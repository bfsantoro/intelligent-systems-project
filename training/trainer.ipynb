{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc615efb",
   "metadata": {},
   "source": [
    "# Product classifier based on its characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150af92",
   "metadata": {},
   "source": [
    "Importing the necessary packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632348ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.environ['DATASET_PATH']\n",
    "METRICS_PATH = os.environ['METRICS_PATH']\n",
    "MODEL_PATH = os.environ['MODEL_PATH']\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original size of dataframe: {df.shape}')\n",
    "df = df.dropna(subset=['price', 'weight', 'minimum_quantity', 'category', 'view_counts'])\n",
    "print(f'Size after dropping NaNs: {df.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843ad2d",
   "metadata": {},
   "source": [
    "The exploratory analysis showed a very large dispersion of product prices. Cases with extremely high prices may be outliers or items with incorrect input data and are thus dropped. The criterion to be an outlier was the usual : 3 standard deviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_filter = (abs(stats.zscore(df.price)<3))\n",
    "df = df[outlier_filter]\n",
    "print(f'Dataframe size after dropping outliers: {df.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe864ce",
   "metadata": {},
   "source": [
    "# Numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fdfc9b",
   "metadata": {},
   "source": [
    "The exploratory analysis showed that numerical features such as express delivery, search page and position would show no correlation with different categories, so only 4 variables were considered. Preliminary tests also showed that the most relevant among these four is the price.\n",
    "\n",
    "Originally, the variables can assume values between 0 and thousands, which can be adjusted with a scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcabef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [df.columns.get_loc(col) for col in ['price', 'weight','minimum_quantity', 'view_counts']]\n",
    "values = df.values[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe91fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(values)\n",
    "values_scaled = scaler.transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfdfda7",
   "metadata": {},
   "source": [
    "# Text features\n",
    "\n",
    "Both Title and the concatenated tags were considered as feature candidates. To avoid an extreme usage of memory, it was necessary to select one or the other (but not both). Among the tested classifiers, the F1 score was usually better if the tags were considered instead of the titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba0499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = np.array(df['concatenated_tags'].tolist())\n",
    "texts = texts.reshape(-1, 1)\n",
    "\n",
    "oh_encoder = OneHotEncoder(sparse=False)\n",
    "texts_encoded = oh_encoder.fit_transform(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = np.concatenate((values_scaled, texts_encoded), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e3dca",
   "metadata": {},
   "source": [
    "# Classifier categories\n",
    "\n",
    "A label encoder was used to convert the categories from text to numerical values. More sophisticated strategies were not tested and this is a point where the model could be improved.\n",
    "\n",
    "The vector of class labels must be reshaped due to the way SGDClassifier works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013dc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.values[:,-1]\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y.ravel())\n",
    "n_dataset = df.shape[0]\n",
    "y_encoded.reshape(n_dataset,)\n",
    "y_encoded = y_encoded.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6627f4c",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "There is a fair amount of data points, so the fraction of the test dataset does not need to be extremely small. The split is random because there is no pattern in data (it is not a time series) but the random state is constant to have reproductible results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cda96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.33, random_state=27)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40694e7e",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Three linear classifiers were evaluated: SVM, logistic regression and one with Huber loss. Empirically, the Huber classifier performed better, probably due to the high dispersion of values in the numerical features. Training time was similar among these classifiers. However, a classifier based on a squared hinge loss function was also considered in a preliminary evaluation but did not finish in a reasonable amount of time (> 40 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_time = time.time()\n",
    "model = SGDClassifier(loss='modified_huber')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "training_time = (time.time()-initial_time)/60\n",
    "print(f\"Training time: {training_time:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = model.predict(X_train)\n",
    "self_accuracy = accuracy_score(y_train, y_train_hat)\n",
    "\n",
    "print(f'Accuracy in self-validation: {100*self_accuracy:.2f} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model.predict(X_test)\n",
    "cross_accuracy = accuracy_score(y_test, y_test_hat)\n",
    "\n",
    "print(f'Accuracy in cross-validation: {100*cross_accuracy:.2f} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e7b93",
   "metadata": {},
   "source": [
    "# Calculating and writing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58544f08-565e-455b-abdb-9e9db057327f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = df.category.drop_duplicates().tolist()\n",
    "precision_average, recall_average, f1_average, _ = precision_recall_fscore_support(y_test, y_test_hat, average='weighted')\n",
    "precision_cat, recall_cat, f1_cat, support_cat = precision_recall_fscore_support(y_test, y_test_hat, average=None, labels=list(range(len(categories))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af4019-f975-4266-8460-c6f8772a6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_cat_writing = dict(zip(categories, precision_cat))\n",
    "recall_cat_writing = dict(zip(categories, recall_cat))\n",
    "f1_cat_writing = dict(zip(categories, f1_cat))\n",
    "support_cat_writing = dict(zip(categories, support_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c35da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = open(METRICS_PATH, \"w\")\n",
    "file.write(f\"Results for Huber loss with {len(y_train)} training data points.\\n\")\n",
    "file.write(f\"Training time: {training_time:.1f} min.\\n\")\n",
    "file.write(f\"Average precision: {100*precision_average:.2f}%.\\n\")\n",
    "file.write(f\"Average recall: {100*recall_average:.2f}%.\\n\")\n",
    "file.write(f\"Average F1: {100*f1_average:.2f}%.\\n\")\n",
    "file.write(\"\\nPrecision (%): \\n\")\n",
    "file.write(str([f\"{c}: {100*precision_cat_writing[c]:.2f}\" for c in categories]))\n",
    "file.write(\"\\nRecall (%): \\n\")\n",
    "file.write(str([f\"{c}: {100*recall_cat_writing[c]:.2f}\" for c in categories]))\n",
    "file.write(\"\\nF1 (%): \\n\")\n",
    "file.write(str([f\"{c}: {100*f1_cat_writing[c]:.2f}\" for c in categories]))\n",
    "file.write(\"\\nNumber of elements per category: \\n\")\n",
    "file.write(str([f\"{c}: {support_cat_writing[c]}\" for c in categories]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e047944",
   "metadata": {},
   "source": [
    "# Final model with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b323f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_encoded, y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b221b9",
   "metadata": {},
   "source": [
    "# Saving the final model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(MODEL_PATH, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
